{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2A7swikevhlzjV192kHxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoHerrig95/Dog_Paw/blob/main/MDN_LSTM_VaR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies"
      ],
      "metadata": {
        "id": "LTpTYwNaDwAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zpzskirzDmVX"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability import distributions as tfd\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mdn # keras-mdn-layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# start date and end date of testing period\n",
        "# shall be the same as in the \"initialisation.R\" script\n",
        "test_start_date = \"2021-01-01\"\n",
        "test_end_date = \"2022-12-31\""
      ],
      "metadata": {
        "id": "uO8DGDXZkvGx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "rYh3zazclEnQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import io\n",
        "#AAPL = pd.read_csv(io.BytesIO(uploaded['AAPL.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "metadata": {
        "id": "IhDzqcuFmTyr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating test data\n",
        "# data subset for testing purposes\n",
        "#test_data = AAPL\n",
        "#test_data[\"date\"] = pd.to_datetime(test_data[\"date\"])\n",
        "#test_data.drop(columns = [\"Unnamed: 0\"], inplace = True)  # dropping redundand columns\n",
        "\n",
        "#AAPL.head()"
      ],
      "metadata": {
        "id": "3bNL_RYHmnrw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data wrangling / train / test\n",
        "\n"
      ],
      "metadata": {
        "id": "brFmMyv68fsu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for converting data tables to input tables\n",
        "# adding past returns (t-i for i [0,d]) and generating new data frame\n",
        "\n",
        "def table_transform(df, rolling_window = 10 ):\n",
        "    # input data frame for training\n",
        "    df_con = df.filter([\"date\",\"R\"]).tail(-1).reset_index(drop = True)\n",
        "\n",
        "    d = rolling_window # window size\n",
        "    location = 2 # value for storage location\n",
        "\n",
        "    # loop for generating columns for past returns\n",
        "    for i in range(1, d+1):\n",
        "        df_con.insert(loc = location, column = \"R_t-\"+str(i), value = np.nan)\n",
        "        location = location + 1\n",
        "\n",
        "    del location\n",
        "\n",
        "    # pasting past returns into columns\n",
        "    for i in range(0, len(df_con) - d):\n",
        "        df_con.iloc[i+d, 2:] = df_con.loc[i:(i+d-1), \"R\"][::-1]\n",
        "\n",
        "\n",
        "    return df_con"
      ],
      "metadata": {
        "id": "xPPtyys48LuX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating a table for testing with d = 100\n",
        "#test = table_transform(df = AAPL, rolling_window = 100)\n",
        "#test = test.tail(-100).reset_index(drop = True) # dropping first 100 rows as they do not include past returns\n",
        "#test.head()\n",
        "#test.info()"
      ],
      "metadata": {
        "id": "mphZOckyVqdN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# defining train set and test set\n",
        "X = test.drop(columns = [\"R\", \"date\"])\n",
        "Y = test.filter([\"R\"])\n",
        "\n",
        "\n",
        "X_train, y_train = X[0:2800], Y[0:2800]\n",
        "X_test , y_test = X[2800: len(X)], Y[2800: len(Y)]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "n3pKWCaVZmGc",
        "outputId": "3780711c-880c-4614-dd5a-8b417d3e35f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# defining train set and test set\\nX = test.drop(columns = [\"R\", \"date\"])\\nY = test.filter([\"R\"])\\n\\n\\nX_train, y_train = X[0:2800], Y[0:2800]\\nX_test , y_test = X[2800: len(X)], Y[2800: len(Y)]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPlAqXiPWtvs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SOd-NaWoNFb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\" A hitchhikers guide to MDN\""
      ],
      "metadata": {
        "id": "iQq9kt03o7Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########### DEPENDENCIES ###########\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "\n",
        "def nnelu(input):\n",
        "    \"\"\" Computes the Non-Negative Exponential Linear Unit\n",
        "    \"\"\"\n",
        "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
        "\n",
        "tf.keras.utils.get_custom_objects().update({'nnelu': Activation(nnelu)})\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "LOSS FUNCTION\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "def slice_parameter_vectors(parameter_vector):\n",
        "    \"\"\" Returns an unpacked list of paramter vectors.\n",
        "    \"\"\"\n",
        "    return [parameter_vector[:,i*components:(i+1)*components] for i in range(no_parameters)]\n",
        "\n",
        "def gnll_loss(y, parameter_vector):\n",
        "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
        "    \"\"\"\n",
        "    alpha, mu, sigma = slice_parameter_vectors(parameter_vector)  # Unpack parameter vectors\n",
        "\n",
        "    gm = tfd.MixtureSameFamily(\n",
        "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
        "        components_distribution=tfd.Normal(\n",
        "            loc=mu,\n",
        "            scale=sigma))\n",
        "\n",
        "    log_likelihood = gm.log_prob(tf.transpose(y))                 # Evaluate log-probability of y\n",
        "\n",
        "    return -tf.reduce_mean(log_likelihood, axis=-1)\n"
      ],
      "metadata": {
        "id": "QyFFEo-po5it"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed Density Network Class"
      ],
      "metadata": {
        "id": "lMn5aOyX8wQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classe for Mixed Density Network\n",
        "\n",
        "class MDN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, neurons=100, components = 2):\n",
        "        super(MDN, self).__init__(name=\"MDN\")\n",
        "        self.neurons = neurons\n",
        "        self.components = components\n",
        "\n",
        "        self.h1 = Dense(neurons, activation=\"relu\", name=\"h1\")\n",
        "        self.h2 = Dense(neurons, activation=\"relu\", name=\"h2\")\n",
        "\n",
        "        self.alphas = Dense(components, activation=\"softmax\", name=\"alphas\")\n",
        "        self.mus = Dense(components, name=\"mus\")\n",
        "        # using nnelu to keep sigma positive\n",
        "        self.sigmas = Dense(components, activation=\"nnelu\", name=\"sigmas\")\n",
        "        self.pvec = Concatenate(name=\"pvec\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.h1(inputs)\n",
        "        x = self.h2(x)\n",
        "\n",
        "        alpha_v = self.alphas(x)\n",
        "        mu_v = self.mus(x)\n",
        "        sigma_v = self.sigmas(x)\n",
        "\n",
        "        return self.pvec([alpha_v, mu_v, sigma_v])"
      ],
      "metadata": {
        "id": "nBJBHfnAmb0_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PARAMETER DECLARATION FOR NEURAL NETWORK\n",
        "-> we generally aim at 2 or 3 distributions\n",
        "2 -> bull / bear\n",
        "3 -> bull / bear / neutral\n",
        "\"\"\"\n",
        "\n",
        "no_parameters = 3 # Paramters of the mixture (alpha, mu, sigma)\n",
        "components = 3 # how many distributions get modelled in the mixture density distribution\n",
        "neurons = 200 # number of neurons"
      ],
      "metadata": {
        "id": "pEQEh84-9D3u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Adam optimisation function\n",
        "opt = tf.optimizers.Adam(1e-3)\n",
        "\n",
        "\n",
        "# no idea what this is\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
        "tensorboard = TensorBoard(log_dir='tbdir/', histogram_freq=0, write_graph=True, write_images=False)\n",
        "mon = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')"
      ],
      "metadata": {
        "id": "lHoGa8KRvFYL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining and compiling the MDN\n",
        "mdn = MDN(neurons=neurons, components=components)\n",
        "mdn.compile(loss=gnll_loss, optimizer=opt)"
      ],
      "metadata": {
        "id": "cH9F4A5hvzw6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXAMPLE DATA NON-LINEAR\n",
        "\"\"\"\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "samples = int(1e4) # number of samples\n",
        "\n",
        "x_data = np.float32(np.random.uniform(-10, 10, (1, samples)))\n",
        "r_data = np.array([np.random.normal(scale=np.abs(i)) for i in x_data])\n",
        "y_data = np.float32(np.square(x_data)+r_data*2.0)\n",
        "\n",
        "x_data2 = np.float32(np.random.uniform(-10, 10, (1, samples)))\n",
        "r_data2 = np.array([np.random.normal(scale=np.abs(i)) for i in x_data2])\n",
        "y_data2 = np.float32(-np.square(x_data2)+r_data2*2.0)\n",
        "\n",
        "x_data = np.concatenate((x_data,x_data2),axis=1).T\n",
        "y_data = np.concatenate((y_data,y_data2),axis=1).T\n",
        "\n",
        "#min_max_scaler = MinMaxScaler()\n",
        "#y_data = min_max_scaler.fit_transform(y_data)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.5, random_state=40, shuffle=True)"
      ],
      "metadata": {
        "id": "Jkk3X0anz5Pl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdn.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=200, callbacks=[mon, tensorboard], batch_size=128, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHp6LH5kv4pK",
        "outputId": "43f7fb3c-6afb-4f2f-df83-b57a7a9ec381"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6e84c65480>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example prediction\n",
        "\n",
        "s = np.linspace(-10,10,int(1e3))[:, np.newaxis].astype(np.float32)\n",
        "y_pred = mdn.predict(s)\n",
        "alpha_pred, mu_pred, sigma_pred = slice_parameter_vectors(y_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97_Ao-QJ2x1O",
        "outputId": "de73fc22-0f73-422c-bede-441c51bd7957"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigma_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhITCYVnCrAc",
        "outputId": "4cb57fd6-ec25-4ade-aab8-bb494e0284c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19.623625, 21.175117, 65.64909 ],\n",
              "       [19.587692, 21.129663, 65.49252 ],\n",
              "       [19.551764, 21.084213, 65.33596 ],\n",
              "       ...,\n",
              "       [17.086039, 22.626158, 23.207914],\n",
              "       [17.110931, 22.67573 , 23.280102],\n",
              "       [17.135832, 22.725302, 23.352293]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6jwOok9GzTl",
        "outputId": "26bfa06e-4bac-443a-f9c2-b3f79ef4949a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.17208439],\n",
              "       [ 6.9451947 ],\n",
              "       [-6.3229437 ],\n",
              "       ...,\n",
              "       [ 7.696569  ],\n",
              "       [ 5.911708  ],\n",
              "       [-7.960221  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session() # clearing model"
      ],
      "metadata": {
        "id": "x2mnTyCDdkcE"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}